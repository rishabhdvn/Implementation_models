{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqgSmX3Ay79BEbMUA08rv1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"viYFzaOdwqvo","executionInfo":{"status":"ok","timestamp":1766905354999,"user_tz":-330,"elapsed":787,"user":{"displayName":"Rishabh Dhawan","userId":"07761211813386037163"}},"outputId":"be20d8fd-6301-4c82-df8b-d2cb81f5cf29"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 0, loss = 5.5459\n","epoch 200, loss = 0.0132\n","epoch 400, loss = 0.0057\n","epoch 600, loss = 0.0036\n","epoch 800, loss = 0.0026\n","epoch 1000, loss = 0.0021\n","epoch 1200, loss = 0.0017\n","epoch 1400, loss = 0.0015\n","epoch 1600, loss = 0.0013\n","epoch 1800, loss = 0.0011\n","\n","Generated text:\n","elloe\n"]}],"source":["# -----------------------------------------\n","# BASIC RNN FROM SCRATCH (WITH EXPLANATIONS)\n","# -----------------------------------------\n","\n","# We ONLY use numpy for math and matrices.\n","import numpy as np\n","\n","# Makes results repeatable (same random numbers each run)\n","np.random.seed(42)\n","\n","\n","# ==========================\n","# 1) DATA PREPARATION\n","# ==========================\n","\n","# Our tiny training text\n","text = \"hello\"\n","\n","# Find all unique characters\n","chars = sorted(list(set(text)))\n","\n","# Number of different characters\n","vocab_size = len(chars)\n","\n","# Map each character -> number (index)\n","char_to_idx = {ch: i for i, ch in enumerate(chars)}\n","\n","# Map number -> character (for later, when we generate)\n","idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n","\n","# Convert text \"hello\" -> [h,e,l,l,o] -> [0,1,2,2,3] (example)\n","data = [char_to_idx[c] for c in text]\n","\n","\n","# ==========================\n","# 2) MODEL SETTINGS\n","# ==========================\n","\n","# Size of the \"memory\" inside RNN\n","hidden_size = 16\n","\n","# How fast weights change during learning\n","learning_rate = 0.1\n","\n","\n","# ==========================\n","# 3) MODEL WEIGHTS\n","# ==========================\n","# These are the values the network will learn.\n","\n","# input -> hidden matrix\n","Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n","\n","# previous hidden -> new hidden (memory loop)\n","Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n","\n","# hidden -> output matrix\n","Why = np.random.randn(vocab_size, hidden_size) * 0.01\n","\n","# bias for hidden layer\n","bh = np.zeros((hidden_size, 1))\n","\n","# bias for output layer\n","by = np.zeros((vocab_size, 1))\n","\n","\n","# ==========================\n","# 4) HELPER FUNCTIONS\n","# ==========================\n","\n","# Turns raw scores -> probabilities (softmax)\n","def softmax(x):\n","    e = np.exp(x - np.max(x))  # numerical stability trick\n","    return e / np.sum(e)\n","\n","# Create a one-hot vector (like [0,0,1,0])\n","def one_hot(idx):\n","    x = np.zeros((vocab_size, 1))\n","    x[idx] = 1\n","    return x\n","\n","\n","# ==========================\n","# 5) FORWARD + BACKWARD PASS (TRAINING STEP)\n","# ==========================\n","# This function:\n","# 1) runs RNN forward\n","# 2) calculates loss\n","# 3) runs backward pass (BPTT)\n","# 4) returns gradients\n","\n","def loss_and_grads(inputs, targets, h_prev):\n","\n","    # Dictionaries to store values for backprop\n","    xs, hs, ys, ps = {}, {}, {}, {}\n","\n","    # Previous hidden state (memory)\n","    hs[-1] = np.copy(h_prev)\n","\n","    loss = 0\n","\n","    # ---------- FORWARD ----------\n","    for t in range(len(inputs)):\n","\n","        # Convert character id -> one-hot vector\n","        xs[t] = one_hot(inputs[t])\n","\n","        # RNN formula:\n","        # h_t = tanh(Wxh*x + Whh*h_prev + b)\n","        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n","\n","        # Raw output scores\n","        ys[t] = np.dot(Why, hs[t]) + by\n","\n","        # Convert to probabilities\n","        ps[t] = softmax(ys[t])\n","\n","        # Cross-entropy loss\n","        loss += -np.log(ps[t][targets[t], 0])\n","\n","    # ---------- BACKWARD ----------\n","    # Initialize all gradients as zero\n","    dWxh = np.zeros_like(Wxh)\n","    dWhh = np.zeros_like(Whh)\n","    dWhy = np.zeros_like(Why)\n","    dbh  = np.zeros_like(bh)\n","    dby  = np.zeros_like(by)\n","\n","    # Gradient flowing backward through time\n","    dh_next = np.zeros_like(hs[0])\n","\n","    # Loop backwards through time\n","    for t in reversed(range(len(inputs))):\n","\n","        # Start from probability output\n","        dy = np.copy(ps[t])\n","\n","        # Subtract 1 from correct class (softmax derivative)\n","        dy[targets[t]] -= 1\n","\n","        # Gradients for Why + output bias\n","        dWhy += np.dot(dy, hs[t].T)\n","        dby  += dy\n","\n","        # Backprop through hidden layer\n","        dh = np.dot(Why.T, dy) + dh_next\n","\n","        # Derivative of tanh\n","        dh_raw = (1 - hs[t] * hs[t]) * dh\n","\n","        # Gradients wrt parameters\n","        dbh  += dh_raw\n","        dWxh += np.dot(dh_raw, xs[t].T)\n","        dWhh += np.dot(dh_raw, hs[t-1].T)\n","\n","        # Pass gradient backward in time\n","        dh_next = np.dot(Whh.T, dh_raw)\n","\n","    # Clip gradients (prevents exploding gradients)\n","    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n","        np.clip(dparam, -5, 5, out=dparam)\n","\n","    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n","\n","\n","# ==========================\n","# 6) TRAIN LOOP\n","# ==========================\n","\n","h_prev = np.zeros((hidden_size, 1))  # initial memory = zeros\n","\n","for epoch in range(2000):\n","\n","    # Inputs: h,e,l,l\n","    inputs  = data[:-1]\n","\n","    # Targets: e,l,l,o\n","    targets = data[1:]\n","\n","    # Run one training step\n","    loss, dWxh, dWhh, dWhy, dbh, dby, h_prev = loss_and_grads(inputs, targets, h_prev)\n","\n","    # Update each parameter using gradient descent\n","    Wxh -= learning_rate * dWxh\n","    Whh -= learning_rate * dWhh\n","    Why -= learning_rate * dWhy\n","    bh  -= learning_rate * dbh\n","    by  -= learning_rate * dby\n","\n","    # Show training progress\n","    if epoch % 200 == 0:\n","        print(f\"epoch {epoch}, loss = {loss:.4f}\")\n","\n","\n","# ==========================\n","# 7) GENERATE TEXT\n","# ==========================\n","# Now letâ€™s see what the model learned.\n","\n","def sample(seed_idx, length=5):\n","\n","    x = one_hot(seed_idx)\n","    h = np.zeros((hidden_size, 1))\n","    output = []\n","\n","    for t in range(length):\n","\n","        # Forward pass through RNN\n","        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n","        y = np.dot(Why, h) + by\n","        p = softmax(y)\n","\n","        # Pick next char based on probabilities\n","        idx = np.random.choice(range(vocab_size), p=p.ravel())\n","\n","        x = one_hot(idx)\n","        output.append(idx_to_char[idx])\n","\n","    return \"\".join(output)\n","\n","\n","print(\"\\nGenerated text:\")\n","print(sample(char_to_idx['h'], 5))\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"oGvwiXymxotw"},"execution_count":null,"outputs":[]}]}